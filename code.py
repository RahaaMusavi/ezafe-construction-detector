# -*- coding: utf-8 -*-
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Feature Extraction for Ezafe Identification\n",
    "\n",
    "**Thesis Chapter:** Corresponds to data preparation described around Chapter 4.\n",
    "\n",
    "**Objective:** This notebook reads annotated linguistic data in CoNLL-U format, identifies nominal heads (nouns, pronouns, proper nouns) and their direct dependents, extracts relevant linguistic features characterizing these relationships, and saves the processed feature set to a CSV file (`nominal_features_cleaned.csv`). This output serves as the input for the Ezafe identification model (Notebook 02)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from conllu import parse_incr\n",
    "from typing import List, Dict, Any, Tuple, Union, Optional, Set\n",
    "import warnings\n",
    "import logging # Added for better messages\n",
    "\n",
    "# Setup basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Suppress specific warnings if needed, but generally good to see them during development\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Constants\n",
    "\n",
    "**Important:** Adjust these paths relative to the notebook's location in your project structure. It's recommended to place data in a `data/` subdirectory and output in an `output/` or `data/` subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Paths --- \n",
    "# Assumes conllu files are in '../data/conllu_files/' relative to this notebook\n",
    "INPUT_FOLDER_PATH = os.path.join('..', 'data', 'conllu_files') \n",
    "# Saves the output CSV to '../data/' relative to this notebook\n",
    "OUTPUT_CSV_PATH = os.path.join('..', 'data', 'nominal_features_cleaned.csv')\n",
    "\n",
    "# --- Feature Extraction Settings --- \n",
    "\n",
    "# Maps observed dependency label variations/typos to their standard form\n",
    "STANDARDIZED_DEPRELS: Dict[str, str] = {\n",
    "    \"nm\": \"nmod\",\n",
    "    \"adjmod\": \"amod\",\n",
    "    \"al:relcl\": \"acl:relcl\",\n",
    "    \"acl:recl\": \"acl:relcl\"\n",
    "    # Add more known variations from your specific dataset here\n",
    "}\n",
    "\n",
    "# Dependency relations to exclude from the analysis entirely\n",
    "STOPWORDS_DEPRELS: Set[str] = {'punct', 'dep', 'reparandum'}\n",
    "\n",
    "# Lemmas identified as potential Ezafe markers (language/annotation specific)\n",
    "POTENTIAL_EZAFE_MARKERS: Set[str] = {'Ä«'} # Example for Persian\n",
    "\n",
    "# Universal POS tags identifying nominal categories to be treated as heads\n",
    "NOMINAL_UPOS_TAGS: Set[str] = {'NOUN', 'PROPN', 'PRON'}\n",
    "\n",
    "# Universal POS tags identifying verbal categories\n",
    "VERBAL_UPOS_TAGS: Set[str] = {'VERB', 'AUX'}\n",
    "\n",
    "# Flag to control adjustment of numerical columns containing zeros\n",
    "# Set to True to add 1 to columns with zeros, False to disable.\n",
    "ADJUST_ZEROS_FLAG = True \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions and Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_deprel(deprel: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Standardizes a dependency label using the STANDARDIZED_DEPRELS mapping.\"\"\"\n",
    "    if deprel is None:\n",
    "        return None\n",
    "    return STANDARDIZED_DEPRELS.get(str(deprel).lower(), str(deprel)) # Added lower() for robustness\n",
    "\n",
    "def convert_token_id_to_float(token_id: Union[int, Tuple[int, str, int], str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Converts CoNLL-U token IDs (int, sub-token tuple like (2,'.',1), numeric str)\n",
    "    to floats. Returns None for range IDs ('1-2') or non-convertible types.\n",
    "    Handles potential errors during conversion.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(token_id, tuple) and len(token_id) == 3: # e.g., (2, '.', 1) for '2.1'\n",
    "            return float(f\"{int(token_id[0])}.{int(token_id[2])}\")\n",
    "        elif isinstance(token_id, (int, float)):\n",
    "            return float(token_id)\n",
    "        elif isinstance(token_id, str):\n",
    "            if '.' in token_id and token_id.replace('.', '', 1).isdigit():\n",
    "                 return float(token_id)\n",
    "            elif token_id.isdigit():\n",
    "                return float(token_id)\n",
    "            else:\n",
    "                # logging.debug(f\"Ignoring non-standard token ID: {token_id}\")\n",
    "                return None # Ignore ranges like '1-2' or other non-numeric strings\n",
    "        else:\n",
    "            # logging.warning(f\"Unexpected token ID type {type(token_id)} ({token_id}).\")\n",
    "            return None\n",
    "    except (ValueError, TypeError, IndexError) as e:\n",
    "         # logging.warning(f\"Could not convert token ID {token_id} to float: {e}\")\n",
    "         return None\n",
    "\n",
    "class Token:\n",
    "    \"\"\"A simple container for relevant CoNLL-U token information.\"\"\"\n",
    "    def __init__(self, id_: float, form: Optional[str], lemma: Optional[str],\n",
    "                 upos: Optional[str], head: Optional[int], deprel: Optional[str]):\n",
    "        self.id: float = id_\n",
    "        self.form: Optional[str] = form\n",
    "        self.lemma: Optional[str] = lemma\n",
    "        self.upos: Optional[str] = upos\n",
    "        self.head: Optional[int] = head # This is the ID of the head token\n",
    "        self.deprel: Optional[str] = deprel # Assumed standardized\n",
    "\n",
    "def adjust_df_zeros(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Adds 1 to all values in numerical columns containing any zeros.\"\"\"\n",
    "    df_adjusted = df.copy()\n",
    "    adjusted_cols_count = 0\n",
    "    num_cols = df_adjusted.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Exclude ID columns explicitly if they exist and are numeric\n",
    "    id_cols_to_exclude = {'nominal_head_id', 'dependent_id'}\n",
    "    cols_to_adjust = [col for col in num_cols if col not in id_cols_to_exclude]\n",
    "    \n",
    "    for col in cols_to_adjust:\n",
    "        # Check if the column contains any zero values\n",
    "        if (df_adjusted[col] == 0).any():\n",
    "            logging.info(f\"Adjusting column '{col}' (+1) due to presence of zeros.\")\n",
    "            df_adjusted[col] = df_adjusted[col] + 1\n",
    "            adjusted_cols_count += 1\n",
    "            \n",
    "    if adjusted_cols_count > 0:\n",
    "        logging.info(f\"Zero adjustment applied to {adjusted_cols_count} numerical column(s).\")\n",
    "    else:\n",
    "        logging.info(\"No zero adjustment needed for numerical columns.\")\n",
    "    return df_adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Core Logic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentence_features(sentence_tokens: List[Token], source_file: str, sentence_id: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extracts features for nominal head-dependent pairs within a single sentence.\"\"\"\n",
    "    nominal_features: List[Dict[str, Any]] = []\n",
    "\n",
    "    # Create a mapping from float token ID to Token object for efficient lookup\n",
    "    token_map = {tok.id: tok for tok in sentence_tokens}\n",
    "    # Create a mapping from integer token ID to Token object for efficient lookup of heads\n",
    "    int_token_map = {int(tok.id): tok for tok in sentence_tokens if tok.id.is_integer()}\n",
    "\n",
    "    for head_token in sentence_tokens:\n",
    "        # Identify nominal heads\n",
    "        if head_token.upos in NOMINAL_UPOS_TAGS:\n",
    "            # Find dependents pointing to this head (using integer ID for head lookup)\n",
    "            head_int_id = int(head_token.id) # Ensure head ID is integer for comparison\n",
    "            dependents = [dep for dep in sentence_tokens if dep.head == head_int_id]\n",
    "            num_dependents_of_head = len(dependents)\n",
    "\n",
    "            for dep_token in dependents:\n",
    "                # Ensure dependent token itself is valid (e.g., has a numeric ID)\n",
    "                if dep_token.id is None: continue\n",
    "                \n",
    "                try:\n",
    "                    # Calculate features\n",
    "                    distance = abs(dep_token.id - head_token.id)\n",
    "                    position = 'before' if dep_token.id < head_token.id else 'after'\n",
    "                    \n",
    "                    # Count dependents of the current dependent token\n",
    "                    dep_int_id = int(dep_token.id)\n",
    "                    num_dependents_of_dependent = sum(1 for t in sentence_tokens if t.head == dep_int_id)\n",
    "                    \n",
    "                    # Check for Ezafe marker related to the dependent\n",
    "                    has_ezafe = any(\n",
    "                        t.lemma in POTENTIAL_EZAFE_MARKERS and t.head == dep_int_id\n",
    "                        for t in sentence_tokens if t.lemma is not None and t.head is not None\n",
    "                    )\n",
    "                    is_verbal = int(dep_token.upos in VERBAL_UPOS_TAGS)\n",
    "\n",
    "                    # Build feature dictionary\n",
    "                    features = {\n",
    "                        # IDs are kept for reference during analysis but likely dropped for modeling\n",
    "                        'nominal_head_id': head_token.id, \n",
    "                        'dependent_id': dep_token.id, \n",
    "                        # Core linguistic features\n",
    "                        'nominal_head_lemma': head_token.lemma, \n",
    "                        'nominal_head_upos': head_token.upos,\n",
    "                        'dependent_lemma': dep_token.lemma, \n",
    "                        'dependent_upos': dep_token.upos,\n",
    "                        'dependent_deprel': dep_token.deprel, \n",
    "                        # Derived features\n",
    "                        'distance': distance, \n",
    "                        'position': position,\n",
    "                        'num_dependents_nominal': num_dependents_of_head, \n",
    "                        'num_dependents_dependent': num_dependents_of_dependent,\n",
    "                        'is_verbal': is_verbal, \n",
    "                        # Source info\n",
    "                        'source_file': source_file,\n",
    "                        'sentence_id': sentence_id,\n",
    "                        # Target variable\n",
    "                        'ezafe_label': int(has_ezafe) # Binary label: 1 if ezafe present, 0 otherwise\n",
    "                    }\n",
    "                    nominal_features.append(features)\n",
    "                except Exception as e:\n",
    "                     # Log error for specific problematic pair but continue\n",
    "                     logging.error(f\"Error extracting features for pair head={head_token.id}, dep={dep_token.id} in {source_file} (sent: {sentence_id}): {e}\")\n",
    "                     continue # Skip this pair\n",
    "\n",
    "    return nominal_features\n",
    "\n",
    "def process_conllu_file(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Reads a CoNLL-U file, processes each sentence, and extracts nominal features.\"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    all_features_in_file: List[Dict[str, Any]] = []\n",
    "    logging.info(f\"Processing file: {file_name}...\")\n",
    "\n",
    "    processed_sentences = 0\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            for sentence_idx, token_list in enumerate(parse_incr(infile)):\n",
    "                sentence_id = token_list.metadata.get('sent_id', f'{file_name}_sent_{sentence_idx+1}')\n",
    "                current_sentence_tokens: List[Token] = []\n",
    "                malformed_token_skipped = False\n",
    "\n",
    "                for token_data in token_list:\n",
    "                    # Basic check for essential fields\n",
    "                    required_keys = {'id', 'form', 'upos', 'head', 'deprel'}\n",
    "                    if not required_keys.issubset(token_data):\n",
    "                         logging.warning(f\"Skipping malformed token data in {sentence_id}: missing keys in {token_data}\")\n",
    "                         malformed_token_skipped = True\n",
    "                         continue\n",
    "                    \n",
    "                    # Convert token ID\n",
    "                    token_id = convert_token_id_to_float(token_data['id'])\n",
    "                    if token_id is None: \n",
    "                        # logging.debug(f\"Skipping token with non-convertible ID {token_data['id']} in {sentence_id}\")\n",
    "                        continue\n",
    "\n",
    "                    # Standardize and filter deprel\n",
    "                    deprel_std = standardize_deprel(token_data['deprel'])\n",
    "                    if deprel_std in STOPWORDS_DEPRELS:\n",
    "                        # logging.debug(f\"Skipping token with stopword deprel '{deprel_std}' in {sentence_id}\")\n",
    "                        continue\n",
    "\n",
    "                    # Convert head ID to integer\n",
    "                    head_id_raw = token_data['head']\n",
    "                    head_id: Optional[int] = None\n",
    "                    if head_id_raw is not None:\n",
    "                        try: \n",
    "                            head_id = int(head_id_raw)\n",
    "                        except (ValueError, TypeError):\n",
    "                             logging.warning(f\"Invalid Head ID '{head_id_raw}' for token {token_id} in {sentence_id}. Skipping token.\")\n",
    "                             malformed_token_skipped = True\n",
    "                             continue # Skip token if head ID is invalid\n",
    "\n",
    "                    # Create Token object\n",
    "                    try:\n",
    "                        tok = Token(\n",
    "                            id_=token_id, \n",
    "                            form=token_data.get('form'), \n",
    "                            lemma=token_data.get('lemma'),\n",
    "                            upos=token_data.get('upos'), \n",
    "                            head=head_id, \n",
    "                            deprel=deprel_std\n",
    "                        )\n",
    "                        current_sentence_tokens.append(tok)\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error creating Token object for {token_data} in {sentence_id}: {e}\")\n",
    "                        malformed_token_skipped = True\n",
    "                        continue\n",
    "\n",
    "                # Process the sentence if it's not empty and no essential tokens were skipped\n",
    "                if current_sentence_tokens and not malformed_token_skipped:\n",
    "                    try:\n",
    "                        sentence_features = extract_sentence_features(current_sentence_tokens, file_name, sentence_id)\n",
    "                        all_features_in_file.extend(sentence_features)\n",
    "                        processed_sentences += 1\n",
    "                    except Exception as e:\n",
    "                         # Catch errors during feature extraction for the sentence\n",
    "                         logging.error(f\"Error processing sentence {sentence_id} features in {file_name}: {e}\")\n",
    "                         # Optionally log traceback: import traceback; traceback.print_exc()\n",
    "                elif malformed_token_skipped:\n",
    "                    logging.warning(f\"Skipped feature extraction for sentence {sentence_id} due to malformed tokens.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found - {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing file {file_path}: {e}\")\n",
    "        # Optionally re-raise for critical errors: raise\n",
    "        return []\n",
    "\n",
    "    logging.info(f\"Finished {file_name}. Processed {processed_sentences} sentences, found {len(all_features_in_file)} nominal head-dependent pairs.\")\n",
    "    return all_features_in_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Processing file: Col_RFS_TD2.conllu...\n",
      "INFO: Finished Col_RFS_TD2.conllu. Processed 12 sentences, found 48 nominal head-dependent pairs.\n",
      "INFO: Processing file: DD-K35.conllu...\n",
      "INFO: Finished DD-K35.conllu. Processed 408 sentences, found 2930 nominal head-dependent pairs.\n",
      "INFO: Processing file: Dk5_B.conllu...\n",
      "INFO: Finished Dk5_B.conllu. Processed 935 sentences, found 6578 nominal head-dependent pairs.\n",
      "INFO: Processing file: Dk7-B.conllu...\n",
      "INFO: Finished Dk7-B.conllu. Processed 265 sentences, found 1839 nominal head-dependent pairs.\n",
      "INFO: Processing file: DMX-L19.conllu...\n",
      "INFO: Finished DMX-L19.conllu. Processed 334 sentences, found 2075 nominal head-dependent pairs.\n",
      "INFO: Processing file: GBd_TD1.conllu...\n",
      "INFO: Finished GBd_TD1.conllu. Processed 1593 sentences, found 11804 nominal head-dependent pairs.\n",
      "INFO: Processing file: NM_K35.conllu...\n",
      "INFO: Finished NM_K35.conllu. Processed 541 sentences, found 3643 nominal head-dependent pairs.\n",
      "INFO: Processing file: NM_TD4a.conllu...\n",
      "INFO: Finished NM_TD4a.conllu. Processed 72 sentences, found 504 nominal head-dependent pairs.\n",
      "INFO: Processing file: RAF-TD2.conllu...\n",
      "INFO: Finished RAF-TD2.conllu. Processed 155 sentences, found 919 nominal head-dependent pairs.\n",
      "INFO: Processing file: RFS-TD2.conllu...\n",
      "INFO: Finished RFS-TD2.conllu. Processed 27 sentences, found 184 nominal head-dependent pairs.\n",
      "INFO: Processing file: ZWY-K20.conllu...\n",
      "INFO: Finished ZWY-K20.conllu. Processed 280 sentences, found 1969 nominal head-dependent pairs.\n",
      "INFO: Processing file: ZWY-K43a.conllu...\n",
      "INFO: Finished ZWY-K43a.conllu. Processed 268 sentences, found 1897 nominal head-dependent pairs.\n",
      "INFO: Successfully processed 12 CoNLL-U files.\n",
      "INFO: Combined DataFrame created with 34380 rows.\n",
      "INFO: Dropped 8688 duplicate rows. DataFrame shape after deduplication: (25692, 16)\n",
      "INFO: Adjusting column 'distance' (+1) due to presence of zeros.\n",
      "INFO: Adjusting column 'num_dependents_nominal' (+1) due to presence of zeros.\n",
      "INFO: Adjusting column 'num_dependents_dependent' (+1) due to presence of zeros.\n",
      "INFO: Adjusting column 'is_verbal' (+1) due to presence of zeros.\n",
      "INFO: Adjusting column 'ezafe_label' (+1) due to presence of zeros.\n",
      "INFO: Zero adjustment applied to 5 numerical column(s).\n",
      "INFO: Nominal features data saved to ../data/nominal_features_cleaned.csv\n",
      "INFO: Final DataFrame has 25692 rows.\n"
     ]
    }
   ],
   "source": [
    "all_extracted_data = []\n",
    "processed_files_count = 0\n",
    "\n",
    "if not os.path.isdir(INPUT_FOLDER_PATH):\n",
    "    logging.error(f\"Input directory not found: {INPUT_FOLDER_PATH}\")\n",
    "else:\n",
    "    conllu_files = sorted([f for f in os.listdir(INPUT_FOLDER_PATH) if f.lower().endswith('.conllu')])\n",
    "    if not conllu_files:\n",
    "        logging.warning(f\"No .conllu files found in {INPUT_FOLDER_PATH}\")\n",
    "    else:\n",
    "        for filename in conllu_files:\n",
    "            file_path = os.path.join(INPUT_FOLDER_PATH, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                try:\n",
    "                    file_features = process_conllu_file(file_path)\n",
    "                    all_extracted_data.extend(file_features)\n",
    "                    processed_files_count += 1\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed to process {filename}: {e}\")\n",
    "        \n",
    "        logging.info(f\"Successfully processed {processed_files_count} CoNLL-U files.\")\n",
    "\n",
    "if all_extracted_data:\n",
    "    features_df = pd.DataFrame(all_extracted_data)\n",
    "    logging.info(f\"Combined DataFrame created with {len(features_df)} rows.\")\n",
    "    \n",
    "    # Data Cleaning\n",
    "    initial_rows = len(features_df)\n",
    "    features_df.drop_duplicates(inplace=True)\n",
    "    rows_dropped = initial_rows - len(features_df)\n",
    "    if rows_dropped > 0:\n",
    "        logging.info(f\"Dropped {rows_dropped} duplicate rows. DataFrame shape after deduplication: {features_df.shape}\")\n",
    "    \n",
    "    # Optional: Adjust numerical columns with zeros \n",
    "    if ADJUST_ZEROS_FLAG:\n",
    "        features_df = adjust_df_zeros(features_df)\n",
    "\n",
    "    # Save the final DataFrame\n",
    "    try:\n",
    "        output_dir = os.path.dirname(OUTPUT_CSV_PATH)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            logging.info(f\"Creating output directory: {output_dir}\")\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        features_df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')\n",
    "        logging.info(f\"Nominal features data saved to {OUTPUT_CSV_PATH}\")\n",
    "        logging.info(f\"Final DataFrame has {len(features_df)} rows.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the DataFrame to CSV: {e}\")\n",
    "else:\n",
    "    logging.warning(\"No data extracted. Output CSV file will not be created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file '../data/nominal_features_cleaned.csv' exists: True\n",
      "First 5 rows of the saved data:\n",
      "   nominal_head_id  dependent_id nominal_head_lemma nominal_head_upos  \\\n",
      "0              2.0           1.0                  ?               ADJ   \n",
      "1              3.0           2.0                  ?               ADJ   \n",
      "2              4.0           3.0                  ?              PROPN   \n",
      "3              5.0           4.0                 be               AUX   \n",
      "4              6.0           5.0                and             CCONJ   \n",
      "\n",
      "  dependent_lemma dependent_upos dependent_deprel  distance position  \\\n",
      "0               ?           NOUN             nmod       1.0   before   \n",
      "1               ?            ADP             case       1.0   before   \n",
      "2               ?           NOUN             nmod       1.0   before   \n",
      "3               ?           NOUN             nsubj      1.0   before   \n",
      "4               ?           VERB              conj      1.0   before   \n",
      "\n",
      "   num_dependents_nominal  num_dependents_dependent  is_verbal  \\\n",
      "0                       1                           1          1   \n",
      "1                       1                           1          1   \n",
      "2                       1                           1          1   \n",
      "3                       3                           1          1   \n",
      "4                       1                           1          2   \n",
      "\n",
      "          source_file        sentence_id  ezafe_label  \n",
      "0  Col_RFS_TD2.conllu  Col_RFS_TD2_sent_1            1  \n",
      "1  Col_RFS_TD2.conllu  Col_RFS_TD2_sent_1            1  \n",
      "2  Col_RFS_TD2.conllu  Col_RFS_TD2_sent_1            1  \n",
      "3  Col_RFS_TD2.conllu  Col_RFS_TD2_sent_1            1  \n",
      "4  Col_RFS_TD2.conllu  Col_RFS_TD2_sent_1            1  \n"
     ]
    }
   ],
   "source": [
    "# Verify the output file exists and optionally check its contents\n",
    "if os.path.exists(OUTPUT_CSV_PATH):\n",
    "    logging.info(f\"Output file '{OUTPUT_CSV_PATH}' exists: True\")\n",
    "    try:\n",
    "        df_check = pd.read_csv(OUTPUT_CSV_PATH)\n",
    "        logging.info(\"First 5 rows of the saved data:\")\n",
    "        print(df_check.head().to_string())\n",
    "        # Optionally check for NaNs or specific values\n",
    "        # print(\"\\nNaN counts per column:\\n\", df_check.isnull().sum())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not read or verify the saved CSV file: {e}\")\n",
    "else:\n",
    "     logging.warning(f\"Output file '{OUTPUT_CSV_PATH}' was not created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}