{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#submitting programs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e8c08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Nominal Feature Extraction ---\n",
      "Reading CoNLL-U files from: C:\\Users\\rahaa\\Dropbox\\MPCD\\conllus_with_erros\n",
      "Found 12 CoNLL-U files to process.\n",
      "Processing: Col_RFS_TD2.conllu...\n",
      "  Finished Col_RFS_TD2.conllu, found 32 nominal pairs.\n",
      "Processing: DD-K35.conllu...\n",
      "  Finished DD-K35.conllu, found 1991 nominal pairs.\n",
      "Processing: DMX-L19.conllu...\n",
      "  Finished DMX-L19.conllu, found 1447 nominal pairs.\n",
      "Processing: Dk5_B.conllu...\n",
      "  Finished Dk5_B.conllu, found 4528 nominal pairs.\n",
      "Processing: Dk7-B.conllu...\n",
      "  Finished Dk7-B.conllu, found 1258 nominal pairs.\n",
      "Processing: GBd_TD1.conllu...\n",
      "  Finished GBd_TD1.conllu, found 8202 nominal pairs.\n",
      "Processing: NM_K35.conllu...\n",
      "  Finished NM_K35.conllu, found 2476 nominal pairs.\n",
      "Processing: NM_TD4a.conllu...\n",
      "  Finished NM_TD4a.conllu, found 378 nominal pairs.\n",
      "Processing: RAF-TD2.conllu...\n",
      "  Finished RAF-TD2.conllu, found 657 nominal pairs.\n",
      "Processing: RFS-TD2.conllu...\n",
      "  Finished RFS-TD2.conllu, found 144 nominal pairs.\n",
      "Processing: ZWY-K20.conllu...\n",
      "  Finished ZWY-K20.conllu, found 1378 nominal pairs.\n",
      "Processing: ZWY-K43a.conllu...\n",
      "  Finished ZWY-K43a.conllu, found 1335 nominal pairs.\n",
      "\n",
      "Consolidating features...\n",
      "Total features extracted (raw): 23826\n",
      "Features after removing duplicates: 23238 (588 removed)\n",
      "Performing zero adjustment on numerical columns...\n",
      "  Adjusting column 'distance' (+1) due to presence of zeros.\n",
      "  Adjusting column 'num_dependents_dependent' (+1) due to presence of zeros.\n",
      "  Adjusting column 'ezafe_label' (+1) due to presence of zeros.\n",
      "  Adjusting column 'is_verbal' (+1) due to presence of zeros.\n",
      "  Zero adjustment applied to 4 column(s).\n",
      "\n",
      "Saving final features (23238 rows) to: C:\\Users\\rahaa\\Dropbox\\MPCD\\LR-input.csv\n",
      "--- Processing complete. Features saved successfully. ---\n",
      "--- Script execution finished. ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Nominal Head Feature Extractor from CoNLL-U Files (preprocessing script for the Logistic Regression model)\n",
    "Author: [Raha Musavi]\n",
    "Date: [2025-05-02]\n",
    "\n",
    "This script processes annotated linguistic data in the CoNLL-U format to extract\n",
    "features relevant to the study of nominal heads (nouns, pronouns, proper nouns)\n",
    "and their direct dependents.\n",
    "\n",
    "\n",
    "Workflow:\n",
    "1.  Reads `.conllu` files from the specified input directory (`INPUT_FOLDER_PATH`).\n",
    "2.  Iterates through each `.conllu` file.\n",
    "3.  Parses each sentence within the files using the `conllu` library.\n",
    "4.  For each token (word), it performs:\n",
    "    a. ID Conversion: Handles standard integer IDs and sub-token IDs (e.g., '2.1').\n",
    "    b. Deprel Standardization: Corrects known typos or variations in dependency\n",
    "       relation labels using a predefined mapping (e.g., 'nm' -> 'nmod').\n",
    "    c. Filtering: Ignores tokens with dependency relations deemed irrelevant\n",
    "       for this analysis (e.g., 'punct', 'dep').\n",
    "5.  Identifies tokens acting as nominal heads (NOUN, PROPN, PRON).\n",
    "6.  For each nominal head, it finds its direct dependents within the same sentence.\n",
    "7.  Extracts features characterizing the head-dependent relationship (see code).\n",
    "8.  Aggregates these features from all files into a pandas DataFrame.\n",
    "9.  Cleans the data by removing exact duplicate feature rows.\n",
    "10. Optionally (controlled by `ADJUST_ZEROS_FLAG`) adjusts numerical columns\n",
    "    containing zeros by adding 1 to all values in that column.\n",
    "11. Saves the final feature set to the specified output CSV file (`OUTPUT_CSV_PATH`).\n",
    "\n",
    "Dependencies:\n",
    "- Python 3.6+\n",
    "- pandas library (`pip install pandas`)\n",
    "- python-conllu library (`pip install conllu`)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from conllu import parse_incr\n",
    "from typing import List, Dict, Any, Tuple, Union, Optional, Set\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# Maps observed dependency label typos to their standard form\n",
    "STANDARDIZED_DEPRELS: Dict[str, str] = {\n",
    "    \"nm\": \"nmod\",\n",
    "    \"adjmod\": \"amod\",\n",
    "    \"al:relcl\": \"acl:relcl\",\n",
    "    \"acl:recl\": \"acl:relcl\",\n",
    "}\n",
    "\n",
    "# Stopwords according to the dependency relations to exclude from the analysis entirely\n",
    "STOPWORDS_DEPRELS: Set[str] = {'punct', 'dep', 'reparandum'}\n",
    "\n",
    "# potential ezafe marker lemma\n",
    "POTENTIAL_EZAFE_MARKERS: Set[str] = {'Ä«'} \n",
    "\n",
    "# Universal POS tags identifying nominal categories to be treated as heads\n",
    "NOMINAL_UPOS_TAGS: Set[str] = {'NOUN', 'PROPN', 'PRON'}\n",
    "\n",
    "# Universal POS tags identifying verbal categories\n",
    "VERBAL_UPOS_TAGS: Set[str] = {'VERB', 'AUX'}\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def standardize_deprel(deprel: Optional[str]) -> Optional[str]:\n",
    "    \"\"\"Standardizes a dependency label using the STANDARDIZED_DEPRELS mapping.\"\"\"\n",
    "    if deprel is None:\n",
    "        return None\n",
    "    return STANDARDIZED_DEPRELS.get(deprel, deprel)\n",
    "\n",
    "def convert_token_id_to_float(token_id: Union[int, Tuple[int, str, int], str]) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Converts CoNLL-U token IDs (int, sub-token tuple like (2,'.',1), numeric str)\n",
    "    to floats. Returns None for range IDs ('1-2') or non-convertible types.\n",
    "    \"\"\"\n",
    "    if isinstance(token_id, tuple) and len(token_id) == 3: # e.g., (2, '.', 1) for '2.1'\n",
    "        try:\n",
    "            return float(f\"{int(token_id[0])}.{int(token_id[2])}\")\n",
    "        except (ValueError, TypeError, IndexError):\n",
    "             print(f\"Warning: Could not convert complex ID tuple {token_id} to float.\")\n",
    "             return None\n",
    "    elif isinstance(token_id, (int, float)):\n",
    "        return float(token_id)\n",
    "    elif isinstance(token_id, str):\n",
    "        if '.' in token_id and token_id.replace('.', '', 1).isdigit():\n",
    "             try: return float(token_id)\n",
    "             except ValueError: return None\n",
    "        elif token_id.isdigit():\n",
    "            try: return float(token_id)\n",
    "            except ValueError: return None\n",
    "        else:\n",
    "            return None # Ignore ranges like '1-2'\n",
    "    print(f\"Warning: Unexpected token ID type {type(token_id)} ({token_id}).\")\n",
    "    return None\n",
    "\n",
    "# --- Data Structure Class ---\n",
    "\n",
    "class Token:\n",
    "    \"\"\"A simple container for relevant CoNLL-U token information.\"\"\"\n",
    "    def __init__(self, id_: float, form: Optional[str], lemma: Optional[str],\n",
    "                 upos: Optional[str], head: Optional[int], deprel: Optional[str]):\n",
    "        self.id: float = id_\n",
    "        self.form: Optional[str] = form\n",
    "        self.lemma: Optional[str] = lemma\n",
    "        self.upos: Optional[str] = upos\n",
    "        self.head: Optional[int] = head\n",
    "        self.deprel: Optional[str] = deprel # Assumed standardized\n",
    "\n",
    "# --- Core Logic Functions ---\n",
    "\n",
    "def extract_np_features(sentence_tokens: List[Token], source_file: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Extracts features for nominal head-dependent pairs within a single sentence.\"\"\"\n",
    "    nominal_features: List[Dict[str, Any]] = []\n",
    "\n",
    "    for head_token in sentence_tokens:\n",
    "        if head_token.upos in NOMINAL_UPOS_TAGS:\n",
    "            dependents = [dep for dep in sentence_tokens if dep.head == int(head_token.id)]\n",
    "            num_dependents_of_head = len(dependents)\n",
    "\n",
    "            for dep_token in dependents:\n",
    "                try:\n",
    "                    distance = abs(dep_token.id - head_token.id)\n",
    "                    position = 'before' if dep_token.id < head_token.id else 'after'\n",
    "                    num_dependents_of_dependent = sum(1 for t in sentence_tokens if t.head == int(dep_token.id))\n",
    "                    has_ezafe = any(\n",
    "                        t.lemma in POTENTIAL_EZAFE_MARKERS and t.head == int(dep_token.id)\n",
    "                        for t in sentence_tokens if t.lemma is not None and t.head is not None\n",
    "                    )\n",
    "                    is_verbal = int(dep_token.upos in VERBAL_UPOS_TAGS)\n",
    "\n",
    "                    features = {\n",
    "                        'nominal_head_id': head_token.id, 'nominal_head_lemma': head_token.lemma, 'nominal_head_upos': head_token.upos,\n",
    "                        'dependent_id': dep_token.id, 'dependent_lemma': dep_token.lemma, 'dependent_upos': dep_token.upos,\n",
    "                        'dependent_deprel': dep_token.deprel, 'distance': distance, 'position': position,\n",
    "                        'num_dependents_nominal': num_dependents_of_head, 'num_dependents_dependent': num_dependents_of_dependent,\n",
    "                        'ezafe_label': int(has_ezafe), 'is_verbal': is_verbal, 'source_file': source_file,\n",
    "                    }\n",
    "                    nominal_features.append(features)\n",
    "                except Exception as e:\n",
    "                     # Print error for specific problematic pair but continue\n",
    "                     print(f\"  Error extracting features for pair head={head_token.id}, dep={dep_token.id} in {source_file}: {e}\")\n",
    "                     continue # Skip this pair\n",
    "\n",
    "    return nominal_features\n",
    "\n",
    "def process_conllu_file(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Reads a CoNLL-U file, processes each sentence, and extracts nominal features.\"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    all_features_in_file: List[Dict[str, Any]] = []\n",
    "    print(f\"Processing: {file_name}...\")\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            for sentence_idx, token_list in enumerate(parse_incr(infile)):\n",
    "                sentence_id = token_list.metadata.get('sent_id', f'file_{file_name}_sent_{sentence_idx+1}')\n",
    "                current_sentence_tokens: List[Token] = []\n",
    "\n",
    "                for token_data in token_list:\n",
    "                    if not all(k in token_data for k in ['id', 'form', 'upos', 'head', 'deprel']):\n",
    "                         print(f\"  Skipping malformed token data in {sentence_id}: {token_data}\")\n",
    "                         continue\n",
    "\n",
    "                    token_id = convert_token_id_to_float(token_data['id'])\n",
    "                    if token_id is None: continue\n",
    "\n",
    "                    deprel_std = standardize_deprel(token_data['deprel'])\n",
    "                    if deprel_std in STOPWORDS_DEPRELS: continue\n",
    "\n",
    "                    head_id_raw = token_data['head']\n",
    "                    head_id: Optional[int] = None\n",
    "                    if head_id_raw is not None:\n",
    "                        try: head_id = int(head_id_raw)\n",
    "                        except (ValueError, TypeError):\n",
    "                             print(f\"  Warning: Invalid Head ID '{head_id_raw}' for token {token_id} in {sentence_id}. Skipping token.\")\n",
    "                             continue # Skip token if head ID is invalid\n",
    "\n",
    "                    tok = Token(\n",
    "                        id_=token_id, form=token_data.get('form'), lemma=token_data.get('lemma'),\n",
    "                        upos=token_data.get('upos'), head=head_id, deprel=deprel_std\n",
    "                    )\n",
    "                    current_sentence_tokens.append(tok)\n",
    "\n",
    "                if current_sentence_tokens:\n",
    "                    try:\n",
    "                        sentence_features = extract_np_features(current_sentence_tokens, file_name)\n",
    "                        all_features_in_file.extend(sentence_features)\n",
    "                    except Exception as e:\n",
    "                         # Catch errors during feature extraction for the sentence\n",
    "                         print(f\"  Error processing sentence {sentence_id} features in {file_name}: {e}\")\n",
    "                         # Optionally log traceback: import traceback; traceback.print_exc()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    print(f\"  Finished {file_name}, found {len(all_features_in_file)} nominal pairs.\")\n",
    "    return all_features_in_file\n",
    "\n",
    "def adjust_df_zeros(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Adds 1 to all values in numerical columns containing any zeros.\"\"\"\n",
    "    df_adjusted = df.copy()\n",
    "    adjusted_cols = 0\n",
    "    num_cols = df_adjusted.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in num_cols:\n",
    "        # Add a check for column existence defensive programming\n",
    "        if col in df_adjusted.columns and (df_adjusted[col] == 0).any():\n",
    "            print(f\"  Adjusting column '{col}' (+1) due to presence of zeros.\")\n",
    "            df_adjusted[col] = df_adjusted[col] + 1\n",
    "            adjusted_cols += 1\n",
    "    if adjusted_cols > 0:\n",
    "        print(f\"  Zero adjustment applied to {adjusted_cols} column(s).\")\n",
    "    else:\n",
    "        print(\"  No zero adjustment needed for numerical columns.\")\n",
    "    return df_adjusted\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# --- Configuration ---\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "INPUT_FOLDER_PATH = r'C:\\Users\\rahaa\\Dropbox\\MPCD\\conllus_with_erros'\n",
    "\n",
    "# Specify the desired path for the output CSV file\n",
    "OUTPUT_CSV_PATH = r'C:\\Users\\rahaa\\Dropbox\\MPCD\\LR-input.csv'\n",
    "\n",
    "# Set to True to add 1 to numerical columns with zeros, False to disable\n",
    "ADJUST_ZEROS_FLAG = True\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# --- Execution ---\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "print(\"--- Starting Nominal Feature Extraction ---\")\n",
    "\n",
    "# --- Input Validation ---\n",
    "if not os.path.isdir(INPUT_FOLDER_PATH):\n",
    "    print(f\"Error: Input folder not found: {INPUT_FOLDER_PATH}\")\n",
    "else:\n",
    "    all_extracted_data = []\n",
    "    print(f\"Reading CoNLL-U files from: {INPUT_FOLDER_PATH}\")\n",
    "\n",
    "    # --- File Processing ---\n",
    "    conllu_files = sorted([f for f in os.listdir(INPUT_FOLDER_PATH) if f.lower().endswith('.conllu')])\n",
    "\n",
    "    if not conllu_files:\n",
    "        print(\"Warning: No .conllu files found in the input folder.\")\n",
    "    else:\n",
    "        print(f\"Found {len(conllu_files)} CoNLL-U files to process.\")\n",
    "        for filename in conllu_files:\n",
    "            file_path = os.path.join(INPUT_FOLDER_PATH, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                file_features = process_conllu_file(file_path)\n",
    "                all_extracted_data.extend(file_features)\n",
    "\n",
    "        if not all_extracted_data:\n",
    "            print(\"Processing complete, but no features were extracted.\")\n",
    "        else:\n",
    "            # --- Data Aggregation and Cleaning ---\n",
    "            print(\"\\nConsolidating features...\")\n",
    "            features_df = pd.DataFrame(all_extracted_data)\n",
    "            print(f\"Total features extracted (raw): {len(features_df)}\")\n",
    "\n",
    "            # Remove duplicates\n",
    "            initial_rows = len(features_df)\n",
    "            features_df.drop_duplicates(inplace=True)\n",
    "            print(f\"Features after removing duplicates: {len(features_df)} ({initial_rows - len(features_df)} removed)\")\n",
    "\n",
    "            # Adjust zeros if requested\n",
    "            if ADJUST_ZEROS_FLAG:\n",
    "                print(\"Performing zero adjustment on numerical columns...\")\n",
    "                features_df = adjust_df_zeros(features_df)\n",
    "            else:\n",
    "                print(\"Skipping zero adjustment.\")\n",
    "\n",
    "            # --- Output ---\n",
    "            print(f\"\\nSaving final features ({len(features_df)} rows) to: {OUTPUT_CSV_PATH}\")\n",
    "            try:\n",
    "                # Ensure output directory exists\n",
    "                output_dir = os.path.dirname(OUTPUT_CSV_PATH)\n",
    "                if output_dir and not os.path.exists(output_dir):\n",
    "                    print(f\"Creating output directory: {output_dir}\")\n",
    "                    os.makedirs(output_dir)\n",
    "\n",
    "                features_df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')\n",
    "                print(\"--- Processing complete. Features saved successfully. ---\")\n",
    "            except IOError as e:\n",
    "                print(f\"Error: Failed to write output file '{OUTPUT_CSV_PATH}'. Check path and permissions.\")\n",
    "                print(f\"Details: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred during file saving: {e}\")\n",
    "\n",
    "print(\"--- Script execution finished. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68167292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Feature Engineering for Logistic Regression ---\n",
      "Loading base features from: C:\\Users\\rahaa\\Dropbox\\MPCD\\LR-input.csv\n",
      "Loaded 23238 rows.\n",
      "Dropping specified columns: ['nominal_head_id', 'dependent_id', 'nominal_head_lemma', 'nominal_head_upos', 'dependent_lemma']\n",
      "Columns remaining: ['dependent_upos', 'dependent_deprel', 'distance', 'position', 'num_dependents_nominal', 'num_dependents_dependent', 'ezafe_label', 'is_verbal', 'source_file']\n",
      "Encoding 'position' numerically ('before': 2, 'after': 1)...\n",
      "Applying One-Hot Encoding to: ['dependent_upos', 'dependent_deprel']\n",
      "Created 66 new columns from One-Hot Encoding.\n",
      "Generating interaction features (UPOS * Position)...\n",
      "Generated 15 interaction features.\n",
      "Numerical features identified for scaling: ['distance', 'num_dependents_nominal', 'num_dependents_dependent', 'is_verbal', 'position_numeric']\n",
      "Applying RobustScaler...\n",
      "RobustScaler applied successfully.\n",
      "\n",
      "Saving engineered features (23238 rows) to: C:\\Users\\rahaa\\Dropbox\\MPCD\\LR-input-engineered.csv\n",
      "--- Feature Engineering complete. Engineered features saved successfully. ---\n",
      "--- Script execution finished. ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Engineering for Logistic Regression Model on Middle Persian ezafe Data\n",
    "\n",
    "Author: [Raha Musavi]\n",
    "Date: [2025-05-02]\n",
    "\n",
    "This script takes the preprocessed nominal features (output from the first script)\n",
    "and performs feature engineering steps specifically tailored for the Logistic\n",
    "Regression model, as described in Chapter 5, Section 5.4 of the thesis.\n",
    "\n",
    "Workflow:\n",
    "1.  Loads the base feature CSV file.\n",
    "2.  Drops columns identified as trivial or redundant for this model\n",
    "    (IDs, lemmata, head UPOS).\n",
    "3.  Encodes the 'position' column numerically.\n",
    "4.  Applies One-Hot Encoding to categorical features ('dependent_upos',\n",
    "    'dependent_deprel').\n",
    "5.  Generates interaction features between encoded 'dependent_upos' and\n",
    "    numerical 'position'.\n",
    "6.  Identifies all numerical features (original + newly created).\n",
    "7.  Applies RobustScaler to standardize numerical features.\n",
    "8.  Saves the final, engineered feature set to a new CSV file.\n",
    "\n",
    "Dependencies:\n",
    "- Python 3.6+\n",
    "- pandas library (`pip install pandas`)\n",
    "- scikit-learn library (`pip install scikit-learn`)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output if desired\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Input file path (output from the first preprocessing script)\n",
    "INPUT_BASE_CSV_PATH = r'C:\\Users\\rahaa\\Dropbox\\MPCD\\LR-input.csv' # <-- ADJUST if needed\n",
    "\n",
    "# Output file path for the engineered features\n",
    "OUTPUT_ENGINEERED_CSV_PATH = r'C:\\Users\\rahaa\\Dropbox\\MPCD\\LR-input-engineered.csv' # <-- ADJUST if needed\n",
    "\n",
    "# Columns to drop based on thesis Section 5.4.1\n",
    "COLUMNS_TO_DROP = [\n",
    "    \"nominal_head_id\",\n",
    "    \"dependent_id\",\n",
    "    \"nominal_head_lemma\",\n",
    "    \"nominal_head_upos\", # Dropped as per 5.4.1\n",
    "    \"dependent_lemma\",\n",
    "    # 'source_file' # Keep source_file for now, might be used later or dropped in model training script\n",
    "]\n",
    "\n",
    "# Categorical columns for One-Hot Encoding (OHE)\n",
    "CATEGORICAL_COLS_OHE = ['dependent_upos', 'dependent_deprel']\n",
    "\n",
    "# Original numerical columns + position\n",
    "BASE_NUMERIC_COLS = [\n",
    "    \"distance\",\n",
    "    \"num_dependents_nominal\",\n",
    "    \"num_dependents_dependent\",\n",
    "    \"is_verbal\" #a simple numeric (0/1) feature\n",
    "]\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting Feature Engineering for Logistic Regression ---\")\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    print(f\"Loading base features from: {INPUT_BASE_CSV_PATH}\")\n",
    "    if not os.path.exists(INPUT_BASE_CSV_PATH):\n",
    "        raise FileNotFoundError(f\"Input file not found: {INPUT_BASE_CSV_PATH}\")\n",
    "    try:\n",
    "        df = pd.read_csv(INPUT_BASE_CSV_PATH)\n",
    "        print(f\"Loaded {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error loading CSV file: {e}\")\n",
    "\n",
    "    # --- 2. Drop Trivial Columns ---\n",
    "    print(f\"Dropping specified columns: {COLUMNS_TO_DROP}\")\n",
    "    df.drop(columns=COLUMNS_TO_DROP, inplace=True, errors='ignore')\n",
    "    print(f\"Columns remaining: {df.columns.tolist()}\")\n",
    "\n",
    "    # --- 3. Encode Position ---\n",
    "    print(\"Encoding 'position' numerically ('before': 2, 'after': 1)...\")\n",
    "    if \"position\" in df.columns:\n",
    "        df['position_numeric'] = df['position'].map({'before': 2, 'after': 1})\n",
    "        # Check for any values that weren't mapped (e.g., NaN or unexpected strings)\n",
    "        if df['position_numeric'].isnull().any():\n",
    "            print(\"Warning: Found null values after mapping 'position'. Filling with a default (e.g., 0 or median) might be needed depending on analysis.\")\n",
    "            # Example fill: df['position_numeric'].fillna(0, inplace=True) # Or another strategy\n",
    "        df.drop(columns=['position'], inplace=True) # Drop original string column\n",
    "    else:\n",
    "        print(\"Warning: 'position' column not found for encoding.\")\n",
    "\n",
    "    # Add the new numeric position column to our list\n",
    "    if 'position_numeric' in df.columns and 'position_numeric' not in BASE_NUMERIC_COLS:\n",
    "        BASE_NUMERIC_COLS.append('position_numeric')\n",
    "\n",
    "    # --- 4. One-Hot Encode Categorical Features ---\n",
    "    print(f\"Applying One-Hot Encoding to: {CATEGORICAL_COLS_OHE}\")\n",
    "    initial_cols = set(df.columns)\n",
    "    df = pd.get_dummies(df, columns=CATEGORICAL_COLS_OHE, prefix=CATEGORICAL_COLS_OHE, dummy_na=False)\n",
    "    new_ohe_cols = list(set(df.columns) - initial_cols)\n",
    "    print(f\"Created {len(new_ohe_cols)} new columns from One-Hot Encoding.\")\n",
    "\n",
    "    # Separate UPOS columns for interaction step\n",
    "    upos_ohe_cols = [col for col in new_ohe_cols if col.startswith('dependent_upos_')]\n",
    "\n",
    "    # --- 5. Generate Interaction Features ---\n",
    "    print(\"Generating interaction features (UPOS * Position)...\")\n",
    "    interaction_feature_names = []\n",
    "    if 'position_numeric' in df.columns and upos_ohe_cols:\n",
    "        for upos_col in upos_ohe_cols:\n",
    "            interaction_col_name = f\"{upos_col}_x_position\"\n",
    "            df[interaction_col_name] = df[upos_col] * df['position_numeric']\n",
    "            interaction_feature_names.append(interaction_col_name)\n",
    "        print(f\"Generated {len(interaction_feature_names)} interaction features.\")\n",
    "    elif 'position_numeric' not in df.columns:\n",
    "         print(\"Skipping interaction generation: 'position_numeric' column not available.\")\n",
    "    else:\n",
    "         print(\"Skipping interaction generation: No UPOS OHE columns found.\")\n",
    "\n",
    "\n",
    "    # --- 6. Identify All Numerical Features for Scaling ---\n",
    "    # Includes original numerics, encoded position, is_verbal, OHE columns (0/1), interactions\n",
    "    print(f\"Numerical features identified for scaling: {BASE_NUMERIC_COLS}\")\n",
    "\n",
    "    # --- 7. Apply RobustScaler ---\n",
    "    print(\"Applying RobustScaler...\")\n",
    "    if all(col in df.columns for col in BASE_NUMERIC_COLS):\n",
    "        scaler = RobustScaler()\n",
    "        df[BASE_NUMERIC_COLS] = scaler.fit_transform(df[BASE_NUMERIC_COLS])\n",
    "        print(\"RobustScaler applied successfully.\")\n",
    "    else:\n",
    "        missing_numeric = [col for col in BASE_NUMERIC_COLS if col not in df.columns]\n",
    "        print(f\"Warning: Could not apply RobustScaler. Missing numerical columns: {missing_numeric}\")\n",
    "\n",
    "\n",
    "    # --- 8. Save Engineered Features ---\n",
    "    print(f\"\\nSaving engineered features ({len(df)} rows) to: {OUTPUT_ENGINEERED_CSV_PATH}\")\n",
    "    try:\n",
    "        # Ensure output directory exists\n",
    "        output_dir = os.path.dirname(OUTPUT_ENGINEERED_CSV_PATH)\n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            print(f\"Creating output directory: {output_dir}\")\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        df.to_csv(OUTPUT_ENGINEERED_CSV_PATH, index=False, encoding='utf-8')\n",
    "        print(\"--- Feature Engineering complete. Engineered features saved successfully. ---\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error: Failed to write output file '{OUTPUT_ENGINEERED_CSV_PATH}'. Check path and permissions.\")\n",
    "        print(f\"Details: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during file saving: {e}\")\n",
    "\n",
    "print(\"--- Script execution finished. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea6f1202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting LR Model - EXACT THESIS REPLICATION WORKFLOW ---\n",
      "Creating results directory: results_thesis_replication_lr_exact\n",
      "Loading base features from: C:\\Users\\rahaa\\Dropbox\\MPCD\\nominal_features_cleaned.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Input file not found: C:\\Users\\rahaa\\Dropbox\\MPCD\\nominal_features_cleaned.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading base features from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINPUT_BASE_CSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(INPUT_BASE_CSV_PATH):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mINPUT_BASE_CSV_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     nominals_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(INPUT_BASE_CSV_PATH)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Input file not found: C:\\Users\\rahaa\\Dropbox\\MPCD\\nominal_features_cleaned.csv"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Logistic Regression Model - EXACT THESIS REPLICATION SCRIPT\n",
    "\n",
    "Author: [Raha Musavi]\n",
    "Date: [2025-05-02]\n",
    "\n",
    "This script replicates the SPECIFIC workflow from the final cell (f5988c27)\n",
    "of the development notebook 'testthemodelsresults.ipynb', which generated\n",
    "the Logistic Regression results reported in the thesis document (Figure 15).\n",
    "\n",
    "Key characteristics replicated from that specific cell:\n",
    "- Input: 'nominal_features_cleaned.csv' (assumed equivalent to LR-input.csv)\n",
    "- Scaler: StandardScaler (NOT RobustScaler)\n",
    "- Feature Selection: Importance-based selection with threshold 0.15 applied\n",
    "  after an initial model fit.\n",
    "- Oversampling: Applied BEFORE the train-test split.\n",
    "- Train/Test Split: 70/30 on the OVERSAMPLED data.\n",
    "- Final Model Base: LogisticRegression(C=10, ...)\n",
    "- Final Hyperparameter Grid: Only tunes 'solver' = ['newton-cg', 'liblinear']\n",
    "- Final CV Folds: 10\n",
    "\n",
    "**Methodological Note:** This script applies oversampling before the train-test\n",
    "split to match the specific methodology that produced the thesis results.\n",
    "Standard best practice usually recommends splitting first, then oversampling\n",
    "only the training set. Evaluation here is performed on the test set derived\n",
    "from the OVERSAMPLED data.\n",
    "\n",
    "Workflow:\n",
    "1.  Loads the base feature dataset ('nominal_features_cleaned.csv').\n",
    "2.  Performs initial feature engineering (position encoding, OHE).\n",
    "3.  Scales numeric features using StandardScaler.\n",
    "4.  Trains an initial model to calculate feature importance.\n",
    "5.  Selects features based on importance threshold (0.15).\n",
    "6.  Applies RandomOverSampler to the feature-selected dataset.\n",
    "7.  Splits the OVERSAMPLED data into training (70%) and testing (30%) sets.\n",
    "8.  Defines the final Logistic Regression model (C=10) and the specific hyperparameter grid (solver only).\n",
    "9.  Uses GridSearchCV with 10-fold cross-validation on the OVERSAMPLED training data.\n",
    "10. Fits the GridSearchCV object.\n",
    "11. Retrieves the best model.\n",
    "12. Makes predictions on the OVERSAMPLED test set.\n",
    "13. Evaluates the model using accuracy, classification report, and confusion matrix.\n",
    "14. Saves results to files, clearly marked as thesis replication.\n",
    "\n",
    "Dependencies:\n",
    "- Python 3.6+\n",
    "- pandas\n",
    "- scikit-learn\n",
    "- imbalanced-learn\n",
    "- numpy\n",
    "- matplotlib\n",
    "- seaborn\n",
    "- json\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import StandardScaler instead of RobustScaler for this specific replication\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# Input file path (MUST match the file used for the thesis results)\n",
    "INPUT_BASE_CSV_PATH = r'C:\\Users\\rahaa\\Dropbox\\MPCD\\nominal_features_cleaned.csv' # ADJUST IF NEEDED\n",
    "\n",
    "# Target column name\n",
    "TARGET_COLUMN = 'ezafe_label'\n",
    "\n",
    "# Train-test split configuration (applied AFTER oversampling)\n",
    "TEST_SET_SIZE = 0.30 # 70/30 split as per thesis script\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Feature Selection Threshold from thesis script\n",
    "IMPORTANCE_THRESHOLD = 0.15\n",
    "\n",
    "# Oversampler configuration\n",
    "OVERSAMPLER = RandomOverSampler(random_state=RANDOM_STATE)\n",
    "\n",
    "# Logistic Regression base model configuration for FINAL tuning\n",
    "# Matching the hardcoded C=10 from the target cell\n",
    "LOGREG_FINAL_MODEL = LogisticRegression(\n",
    "    C=10, # Hardcoded C=10 as per thesis script\n",
    "    max_iter=1000, # Increase max_iter for robustness\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# GridSearchCV configuration - ONLY tuning solver as per thesis script\n",
    "PARAM_GRID = {\n",
    "    'solver': ['newton-cg', 'liblinear'] # Exact grid from target cell\n",
    "}\n",
    "CV_FOLDS = 10 # 10 folds as per thesis script\n",
    "GRID_SEARCH_CV = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Paths for saving results\n",
    "RESULTS_DIR = 'results_thesis_replication_lr_exact' # New distinct directory name\n",
    "REPORT_FILE_PATH = os.path.join(RESULTS_DIR, 'lr_evaluation_report_thesis_exact.txt')\n",
    "PARAMS_FILE_PATH = os.path.join(RESULTS_DIR, 'lr_best_params_thesis_exact.json')\n",
    "CM_PLOT_FILE_PATH = os.path.join(RESULTS_DIR, 'lr_confusion_matrix_thesis_exact.png')\n",
    "CM_DATA_FILE_PATH = os.path.join(RESULTS_DIR, 'lr_confusion_matrix_data_thesis_exact.csv')\n",
    "IMPORTANCE_CSV_PATH = os.path.join(RESULTS_DIR, \"lr_feature_importance_thesis_exact.csv\")\n",
    "SELECTED_FEATURES_PATH = os.path.join(RESULTS_DIR, \"lr_selected_features_thesis_exact.txt\") # Save selected features\n",
    "\n",
    "# --- Main Script ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Starting LR Model - EXACT THESIS REPLICATION WORKFLOW ---\")\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # --- Create results directory ---\n",
    "    if not os.path.exists(RESULTS_DIR):\n",
    "        print(f\"Creating results directory: {RESULTS_DIR}\")\n",
    "        os.makedirs(RESULTS_DIR)\n",
    "\n",
    "    # --- 1. Load Data ---\n",
    "    print(f\"Loading base features from: {INPUT_BASE_CSV_PATH}\")\n",
    "    if not os.path.exists(INPUT_BASE_CSV_PATH):\n",
    "        raise FileNotFoundError(f\"Input file not found: {INPUT_BASE_CSV_PATH}\")\n",
    "    try:\n",
    "        nominals_df = pd.read_csv(INPUT_BASE_CSV_PATH)\n",
    "        print(f\"Loaded {len(nominals_df)} rows.\")\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error loading CSV file: {e}\")\n",
    "\n",
    "    # --- 2. Initial Feature Engineering ---\n",
    "    print(\"Performing initial feature engineering...\")\n",
    "    nominals_df.drop(columns=[\"nominal_head_id\", \"dependent_id\"], inplace=True, errors='ignore')\n",
    "\n",
    "    if \"position\" in nominals_df.columns:\n",
    "        nominals_df['position_numeric'] = nominals_df['position'].map({'before': 2, 'after': 1}).fillna(0) # Added fillna\n",
    "        nominals_df.drop(columns=['position'], inplace=True, errors='ignore') # Added errors='ignore'\n",
    "    else: print(\"Warning: 'position' column not found.\")\n",
    "\n",
    "    initial_numeric_features = [\"distance\", \"num_dependents_nominal\", \"num_dependents_dependent\", \"position_numeric\"]\n",
    "\n",
    "    # OHE dependent_deprel (using OneHotEncoder as per target cell)\n",
    "    deprel_columns = []\n",
    "    if \"dependent_deprel\" in nominals_df.columns:\n",
    "        deprel_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, drop=None)\n",
    "        nominals_df['dependent_deprel'].fillna('missing_deprel', inplace=True)\n",
    "        deprel_encoded = deprel_encoder.fit_transform(nominals_df[[\"dependent_deprel\"]])\n",
    "        deprel_columns = deprel_encoder.get_feature_names_out() # Correct usage\n",
    "        deprel_df = pd.DataFrame(deprel_encoded, columns=deprel_columns, index=nominals_df.index)\n",
    "        nominals_df = pd.concat([nominals_df.drop(columns=['dependent_deprel']), deprel_df], axis=1)\n",
    "        print(f\"Created {len(deprel_columns)} deprel OHE columns.\")\n",
    "    else: print(\"Warning: 'dependent_deprel' not found.\")\n",
    "\n",
    "    # OHE dependent_upos (using get_dummies as per target cell)\n",
    "    upos_columns = []\n",
    "    if \"dependent_upos\" in nominals_df.columns:\n",
    "        nominals_df['dependent_upos'].fillna('missing_upos', inplace=True)\n",
    "        upos_dummies = pd.get_dummies(nominals_df[\"dependent_upos\"], prefix=\"dependent_upos\", dummy_na=False)\n",
    "        nominals_df = pd.concat([nominals_df.drop(columns=['dependent_upos']), upos_dummies], axis=1)\n",
    "        upos_columns = list(upos_dummies.columns)\n",
    "        print(f\"Created {len(upos_columns)} UPOS OHE columns.\")\n",
    "    else: print(\"Warning: 'dependent_upos' not found.\")\n",
    "\n",
    "    # Interaction Terms (for *all* UPOS dummies as per target cell)\n",
    "    interaction_term_names = []\n",
    "    if 'position_numeric' in nominals_df.columns and upos_columns:\n",
    "        interaction_terms = pd.DataFrame(index=nominals_df.index)\n",
    "        for col in upos_columns:\n",
    "            interaction_col_name = f\"{col}_position_interaction\"\n",
    "            interaction_terms[interaction_col_name] = nominals_df[col] * nominals_df[\"position_numeric\"]\n",
    "            interaction_term_names.append(interaction_col_name)\n",
    "        nominals_df = pd.concat([nominals_df, interaction_terms], axis=1)\n",
    "        print(f\"Generated {len(interaction_term_names)} interaction features.\")\n",
    "    else: print(\"Skipping interaction generation due to missing columns.\")\n",
    "\n",
    "    # --- Define X and y (before feature selection) ---\n",
    "    print(\"Defining initial feature set and target...\")\n",
    "    if TARGET_COLUMN not in nominals_df.columns:\n",
    "        raise KeyError(f\"Target column '{TARGET_COLUMN}' not found.\")\n",
    "    y = nominals_df[TARGET_COLUMN]\n",
    "\n",
    "    # Define features including 'is_verbal' if present\n",
    "    base_feature_cols = [col for col in initial_numeric_features if col in nominals_df.columns]\n",
    "    if \"is_verbal\" in nominals_df.columns: base_feature_cols.append(\"is_verbal\")\n",
    "    else: print(\"Warning: 'is_verbal' column not found.\")\n",
    "\n",
    "    feature_cols_for_X = (\n",
    "         base_feature_cols\n",
    "         + upos_columns\n",
    "         + interaction_term_names\n",
    "         + list(deprel_columns)\n",
    "    )\n",
    "    # Ensure columns exist and remove duplicates\n",
    "    feature_cols_for_X = [col for col in pd.unique(feature_cols_for_X) if col in nominals_df.columns]\n",
    "    X_full = nominals_df[feature_cols_for_X].copy()\n",
    "\n",
    "    # Drop remaining non-numeric columns (like lemmas)\n",
    "    non_numeric_in_X = X_full.select_dtypes(exclude=np.number).columns\n",
    "    if not non_numeric_in_X.empty:\n",
    "        print(f\"Dropping non-numeric columns found before scaling: {non_numeric_in_X.tolist()}\")\n",
    "        X_full.drop(columns=non_numeric_in_X, inplace=True)\n",
    "\n",
    "    # Identify numeric cols for scaling *within X_full*\n",
    "    numeric_features_to_scale = [col for col in initial_numeric_features if col in X_full.columns]\n",
    "\n",
    "    # --- 3. Scale Numeric Features (using StandardScaler) ---\n",
    "    print(f\"Applying StandardScaler to: {numeric_features_to_scale}\")\n",
    "    if numeric_features_to_scale:\n",
    "        # Check/Handle NaN/Inf before scaling\n",
    "        numeric_subset = X_full[numeric_features_to_scale]\n",
    "        if numeric_subset.isnull().values.any() or np.isinf(numeric_subset.values).any():\n",
    "             print(f\"Warning: NaN/inf values found before scaling. Filling with 0.\")\n",
    "             for col in numeric_features_to_scale:\n",
    "                  X_full[col] = X_full[col].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        scaler = StandardScaler() # Using StandardScaler as per target cell\n",
    "        X_full.loc[:, numeric_features_to_scale] = scaler.fit_transform(X_full[numeric_features_to_scale])\n",
    "    else: print(\"Warning: No numeric features to scale.\")\n",
    "\n",
    "    # --- 4. Initial Model for Feature Importance ---\n",
    "    print(\"Training initial model for feature importance...\")\n",
    "    # Final check/fill NaN/Inf\n",
    "    if X_full.isnull().values.any() or np.isinf(X_full.values).any():\n",
    "        print(f\"Warning: NaN/inf values found before initial fit. Filling with 0.\")\n",
    "        X_full = X_full.fillna(0)\n",
    "    initial_model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE)\n",
    "    try:\n",
    "        initial_model.fit(X_full, y)\n",
    "    except Exception as e: print(f\"Error fitting initial model: {e}\"); raise e\n",
    "\n",
    "    # --- 5. Importance-Based Feature Selection ---\n",
    "    print(f\"Selecting features with absolute importance > {IMPORTANCE_THRESHOLD}...\")\n",
    "    if hasattr(initial_model, 'coef_'):\n",
    "        feature_importance = pd.DataFrame({\n",
    "            \"Feature\": X_full.columns, \"Coefficient Value\": initial_model.coef_.flatten()})\n",
    "        feature_importance[\"Absolute Importance\"] = feature_importance[\"Coefficient Value\"].abs()\n",
    "        feature_importance = feature_importance.sort_values(by=\"Absolute Importance\", ascending=False).round(5)\n",
    "        selected_features = feature_importance[feature_importance[\"Absolute Importance\"] > IMPORTANCE_THRESHOLD][\"Feature\"].tolist()\n",
    "        if not selected_features:\n",
    "            print(f\"Warning: No features met importance threshold {IMPORTANCE_THRESHOLD}! Using all features.\")\n",
    "            selected_features = X_full.columns.tolist()\n",
    "        else: print(f\"Selected {len(selected_features)} features out of {X_full.shape[1]}.\")\n",
    "        X_final = X_full[selected_features].copy() # Use .copy()\n",
    "        # Save importance and selected features\n",
    "        try:\n",
    "            feature_importance.to_csv(IMPORTANCE_CSV_PATH, index=False)\n",
    "            print(f\"Feature importance saved to: {IMPORTANCE_CSV_PATH}\")\n",
    "            with open(SELECTED_FEATURES_PATH, 'w') as f:\n",
    "                for feature in selected_features: f.write(f\"{feature}\\n\")\n",
    "            print(f\"Selected feature list saved to: {SELECTED_FEATURES_PATH}\")\n",
    "        except Exception as e: print(f\"Error saving importance/selection files: {e}\")\n",
    "    else:\n",
    "        print(\"Error: Initial model has no coefficients. Cannot perform importance selection.\")\n",
    "        X_final = X_full # Fallback to using all features\n",
    "\n",
    "    # --- 6. Apply Oversampling BEFORE Train/Test Split ---\n",
    "    print(\"Applying RandomOverSampler to the feature-selected dataset...\")\n",
    "    X_resampled, y_resampled = OVERSAMPLER.fit_resample(X_final, y)\n",
    "    print(f\"Resampled dataset size: {X_resampled.shape[0]} samples\")\n",
    "    print(\"Value counts in resampled target:\\n\", pd.Series(y_resampled).value_counts())\n",
    "\n",
    "    # --- 7. Split OVERSAMPLED Data (Train/Test) ---\n",
    "    print(f\"Splitting OVERSAMPLED data into Train (70%) / Test (30%) sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_resampled, y_resampled, test_size=TEST_SET_SIZE, random_state=RANDOM_STATE)\n",
    "    print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "    # --- 8 & 9. Setup and Run GridSearchCV ---\n",
    "    print(f\"\\nStarting GridSearchCV with {CV_FOLDS}-fold CV (tuning only solver)...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=LOGREG_FINAL_MODEL, # Uses C=10 base model\n",
    "        param_grid=PARAM_GRID,        # Uses solver-only grid\n",
    "        cv=GRID_SEARCH_CV,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1, verbose=1, refit=True)\n",
    "\n",
    "    # --- 10. Fit GridSearchCV ---\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # --- 11. Get Best Model and Results ---\n",
    "    print(\"\\nGridSearchCV finished.\")\n",
    "    best_params = grid_search.best_params_\n",
    "    best_cv_score = grid_search.best_score_\n",
    "    # Note: best_params will only contain 'solver' here\n",
    "    print(f\"Best parameters found: {{'C': 10, **best_params}}\") # Manually add C=10 for clarity\n",
    "    print(f\"Best cross-validation accuracy score: {best_cv_score:.4f}\")\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # --- 12. Make Predictions on OVERSAMPLED Test Set ---\n",
    "    print(\"\\nMaking predictions on the OVERSAMPLED test set...\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    # --- 13. Evaluate Model ---\n",
    "    print(\"\\n--- Evaluation Results on OVERSAMPLED Test Set ---\")\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test Accuracy (on oversampled test set): {test_accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report (on oversampled test set):\")\n",
    "    report_str = classification_report(y_test, y_pred, target_names=['Ezafe Absent (0)', 'Ezafe Present (1)'])\n",
    "    print(report_str)\n",
    "    print(\"\\nConfusion Matrix (on oversampled test set):\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    # --- 14. Save Results to Files ---\n",
    "    print(f\"\\n--- Saving Evaluation Results to '{RESULTS_DIR}' ---\")\n",
    "    # (Saving logic - same as before, using variables derived above)\n",
    "    # Ensure results directory exists\n",
    "    if not os.path.exists(RESULTS_DIR):\n",
    "        print(f\"Creating results directory: {RESULTS_DIR}\")\n",
    "        os.makedirs(RESULTS_DIR)\n",
    "    try:\n",
    "        with open(REPORT_FILE_PATH, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"Logistic Regression Model Evaluation Report (Exact Thesis Replication Workflow)\\n\")\n",
    "            f.write(\"===============================================================================\\n\\n\")\n",
    "            f.write(\"** WARNING: Evaluation performed on test split derived from OVERSAMPLED data, matching specific thesis methodology. **\\n\\n\")\n",
    "            f.write(f\"Input Data: {INPUT_BASE_CSV_PATH}\\n\")\n",
    "            f.write(f\"Feature Selection Threshold: {IMPORTANCE_THRESHOLD}\\n\")\n",
    "            f.write(f\"Number of Selected Features: {len(selected_features)}\\n\\n\")\n",
    "            f.write(f\"Best Hyperparameters Found by GridSearchCV (C fixed at 10):\\n{{'C': 10, **best_params}}\\n\\n\") # Reflect fixed C\n",
    "            f.write(f\"Best Cross-Validation Accuracy Score: {best_cv_score:.4f}\\n\\n\")\n",
    "            f.write(f\"Test Set Accuracy (on oversampled split): {test_accuracy:.4f}\\n\\n\")\n",
    "            f.write(\"Test Set Classification Report (on oversampled split):\\n\")\n",
    "            f.write(report_str)\n",
    "            f.write(\"\\n\\nTest Set Confusion Matrix (on oversampled split):\\n\")\n",
    "            f.write(np.array2string(cm, separator=', '))\n",
    "        print(f\"Evaluation report saved to: {REPORT_FILE_PATH}\")\n",
    "    except Exception as e: print(f\"Error saving text report: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Save the actual best params found (solver) along with the fixed C\n",
    "        final_params_to_save = {'C': 10, **best_params}\n",
    "        with open(PARAMS_FILE_PATH, 'w', encoding='utf-8') as f: json.dump(final_params_to_save, f, indent=4)\n",
    "        print(f\"Best parameters saved to: {PARAMS_FILE_PATH}\")\n",
    "    except Exception as e: print(f\"Error saving parameters JSON: {e}\")\n",
    "\n",
    "    try:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=['Predicted Absent (0)', 'Predicted Present (1)'],\n",
    "                    yticklabels=['Actual Absent (0)', 'Actual Present (1)'])\n",
    "        plt.ylabel('Actual Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.title('Confusion Matrix - Oversampled Test Set (Thesis Rep)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(CM_PLOT_FILE_PATH, dpi=300)\n",
    "        print(f\"Confusion matrix plot saved to: {CM_PLOT_FILE_PATH}\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    except ImportError: print(\"\\n(Install matplotlib and seaborn to save plot)\")\n",
    "    except Exception as e: print(f\"Error saving confusion matrix plot: {e}\")\n",
    "\n",
    "    try:\n",
    "        cm_df = pd.DataFrame(cm, index=['Actual Absent (0)', 'Actual Present (1)'], columns=['Predicted Absent (0)', 'Predicted Present (1)'])\n",
    "        cm_df.to_csv(CM_DATA_FILE_PATH)\n",
    "        print(f\"Confusion matrix data saved to: {CM_DATA_FILE_PATH}\")\n",
    "    except Exception as e: print(f\"Error saving confusion matrix data: {e}\")\n",
    "\n",
    "    print(\"\\n--- Script execution finished. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fbad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
